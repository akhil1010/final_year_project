{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akhil Upadhyay\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\ttrain's auc: 0.915979\n",
      "[100]\ttrain's auc: 0.98844\n",
      "[150]\ttrain's auc: 0.975784\n",
      "[200]\ttrain's auc: 0.990421\n",
      "[250]\ttrain's auc: 0.988381\n",
      "[300]\ttrain's auc: 0.972276\n",
      "[350]\ttrain's auc: 0.991834\n",
      "[400]\ttrain's auc: 0.985618\n",
      "[450]\ttrain's auc: 0.996303\n",
      "[500]\ttrain's auc: 0.998032\n",
      "[550]\ttrain's auc: 0.994295\n",
      "[600]\ttrain's auc: 0.989301\n",
      "[650]\ttrain's auc: 0.987821\n",
      "======================================================================\n",
      "============================ Final Report ============================\n",
      "======================================================================\n",
      "2019-11-21 12:42:31.562940 \n",
      "\n",
      "   train time     : 0:08:34.072317\n",
      "   output file    : submission.csv\n",
      "    train auc     : 0.98782\n",
      "   MAX_ROUNDS     : 1000\n",
      "   EARLY_STOP     : 50\n",
      "   OPT_ROUNDS     : 0\n",
      "\n",
      "    skiprows      : range(1, 70000)\n",
      "      nrows       : 30000\n",
      "\n",
      "    variables     : ['app', 'device', 'os', 'channel', 'hour', 'nip_day_test_hh', 'nip_day_hh', 'nip_hh_os', 'nip_hh_app', 'nip_hh_dev']\n",
      "   categorical    : ['app', 'device', 'os', 'channel', 'hour']\n",
      "\n",
      "  model params    : {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'learning_rate': 0.1, 'num_leaves': 9, 'max_depth': 5, 'min_child_samples': 100, 'max_bin': 100, 'subsample': 0.9, 'subsample_freq': 1, 'colsample_bytree': 0.7, 'min_child_weight': 0, 'min_split_gain': 0, 'nthread': 8, 'verbose': 0, 'scale_pos_weight': 99.7}\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/tetyanayatsenko/small-adition-to-single-lgbm-lb-0-9893\n",
    "# most_freq_hours_in_test_data    = [4, 5, 9, 10, 13, 14]\n",
    "# middle1_freq_hours_in_test_data = [16, 17, 22] I added one mo level to freq. level. So i reveived 0.9893 instead og 0.9892\n",
    "# least_freq_hours_in_test_data   = [6, 11, 15]\n",
    "\n",
    "# This model is based on the kernel of  Pranav Pandya and andy harless.\n",
    "#       https://www.kaggle.com/pranav84/single-lightgbm-in-r-with-75-mln-rows-lb-0-9690\n",
    "#       https://www.kaggle.com/aharless/try-pranav-s-r-lgbm-in-python/code\n",
    "# I modified code to make it easier for me\n",
    "# The actual changed values are the parameters only. I focused on parameter tuning\n",
    "# num_leaves :  7  ->  9\n",
    "# max_depth  :  4  ->  5\n",
    "# subsample  : 0.7 -> 0.9\n",
    "\n",
    "\n",
    "# This is the first version and will be performing additional data sampling and parameter tuning.\n",
    "\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split # for validation \n",
    "import lightgbm as lgb\n",
    "import gc # memory \n",
    "from datetime import datetime # train time checking\n",
    "\n",
    "\n",
    "\n",
    "start = datetime.now()\n",
    "VALIDATE = False\n",
    "RANDOM_STATE = 50\n",
    "VALID_SIZE = 0.90\n",
    "MAX_ROUNDS = 1000\n",
    "EARLY_STOP = 50\n",
    "OPT_ROUNDS = 650\n",
    "skiprows = range(1,70000)\n",
    "nrows = 30000\n",
    "output_filename = 'submission.csv'\n",
    "\n",
    "path = '../input/'\n",
    "\n",
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        'click_id'      : 'uint32'\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "train_cols = ['ip','app','device','os', 'channel', 'click_time', 'is_attributed']\n",
    "train_df = pd.read_csv(path+\"train.csv\", skiprows=skiprows, nrows=nrows,dtype=dtypes, usecols=train_cols)\n",
    "\n",
    "len_train = len(train_df)\n",
    "gc.collect()\n",
    "\n",
    "most_freq_hours_in_test_data    = [4, 5, 9, 10, 13, 14]\n",
    "middle1_freq_hours_in_test_data = [16, 17, 22]\n",
    "least_freq_hours_in_test_data   = [6, 11, 15]\n",
    "\n",
    "def prep_data( df ):\n",
    "    \n",
    "    df['hour'] = pd.to_datetime(df.click_time).dt.hour.astype('uint8')\n",
    "    df['day'] = pd.to_datetime(df.click_time).dt.day.astype('uint8')\n",
    "    df.drop(['click_time'], axis=1, inplace=True)\n",
    "    gc.collect()\n",
    "    \n",
    "    df['in_test_hh'] = (   4 \n",
    "                         - 3*df['hour'].isin(  most_freq_hours_in_test_data ) \n",
    "                         - 2*df['hour'].isin(  middle1_freq_hours_in_test_data ) \n",
    "                         - 1*df['hour'].isin( least_freq_hours_in_test_data ) ).astype('uint8')\n",
    "    gp = df[['ip', 'day', 'in_test_hh', 'channel']].groupby(by=['ip', 'day', 'in_test_hh'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'nip_day_test_hh'})\n",
    "    df = df.merge(gp, on=['ip','day','in_test_hh'], how='left')\n",
    "    df.drop(['in_test_hh'], axis=1, inplace=True)\n",
    "    df['nip_day_test_hh'] = df['nip_day_test_hh'].astype('uint32')\n",
    "   \n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    gp = df[['ip', 'day', 'hour', 'channel']].groupby(by=['ip', 'day', 'hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'nip_day_hh'})\n",
    "    df = df.merge(gp, on=['ip','day','hour'], how='left')\n",
    "    df['nip_day_hh'] = df['nip_day_hh'].astype('uint16')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    \n",
    "    gp = df[['ip', 'os', 'hour', 'channel']].groupby(by=['ip', 'os', 'hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'nip_hh_os'})\n",
    "    df = df.merge(gp, on=['ip','os','hour'], how='left')\n",
    "    df['nip_hh_os'] = df['nip_hh_os'].astype('uint16')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    gp = df[['ip', 'app', 'hour', 'channel']].groupby(by=['ip', 'app',  'hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'nip_hh_app'})\n",
    "    df = df.merge(gp, on=['ip','app','hour'], how='left')\n",
    "    df['nip_hh_app'] = df['nip_hh_app'].astype('uint16')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    gp = df[['ip', 'device', 'hour', 'channel']].groupby(by=['ip', 'device', 'hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'nip_hh_dev'})\n",
    "    df = df.merge(gp, on=['ip','device','hour'], how='left')\n",
    "    df['nip_hh_dev'] = df['nip_hh_dev'].astype('uint32')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    df.drop( ['ip','day'], axis=1, inplace=True )\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "train_df = prep_data(train_df)\n",
    "gc.collect()\n",
    "\n",
    "params = {\n",
    "          'boosting_type': 'gbdt',\n",
    "          'objective': 'binary',\n",
    "          'metric':'auc',\n",
    "          'learning_rate': 0.1,\n",
    "          'num_leaves': 9,  # we should let it be smaller than 2^(max_depth)\n",
    "          'max_depth': 5,  # -1 means no limit\n",
    "          'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "          'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "          'subsample': 0.9,  # Subsample ratio of the training instance.\n",
    "          'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "          'colsample_bytree': 0.7,  # Subsample ratio of columns when constructing each tree.\n",
    "          'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "          'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "          'nthread': 8,\n",
    "          'verbose': 0,\n",
    "          'scale_pos_weight':99.7, # because training data is extremely unbalanced \n",
    "         }\n",
    "\n",
    "target = 'is_attributed'\n",
    "predictors = ['app','device','os', 'channel', 'hour', 'nip_day_test_hh', 'nip_day_hh', 'nip_hh_os', 'nip_hh_app', 'nip_hh_dev']\n",
    "categorical = ['app', 'device', 'os', 'channel', 'hour']\n",
    "\n",
    "\n",
    "if VALIDATE:\n",
    "\n",
    "    train_df, val_df = train_test_split(train_df, test_size=VALID_SIZE, random_state=RANDOM_STATE, shuffle=True )\n",
    "    dtrain = lgb.Dataset(train_df[predictors].values, \n",
    "                         label=train_df[target].values,\n",
    "                         feature_name=predictors,\n",
    "                         categorical_feature=categorical)\n",
    "    del train_df\n",
    "    gc.collect()\n",
    "\n",
    "    dvalid = lgb.Dataset(val_df[predictors].values,\n",
    "                         label=val_df[target].values,\n",
    "                         feature_name=predictors,\n",
    "                         categorical_feature=categorical)\n",
    "    del val_df\n",
    "    gc.collect()\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    model = lgb.train(params, \n",
    "                      dtrain, \n",
    "                      valid_sets=[dtrain, dvalid], \n",
    "                      valid_names=['train','valid'], \n",
    "                      evals_result=evals_results, \n",
    "                      num_boost_round=MAX_ROUNDS,\n",
    "                      early_stopping_rounds=EARLY_STOP,\n",
    "                      verbose_eval=50, \n",
    "                      feval=None)\n",
    "\n",
    "    del dvalid\n",
    "\n",
    "else:\n",
    "\n",
    "    gc.collect()\n",
    "    dtrain = lgb.Dataset(train_df[predictors].values, label=train_df[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical\n",
    "                          )\n",
    "    del train_df\n",
    "    gc.collect()\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    model = lgb.train(params, \n",
    "                      dtrain, \n",
    "                      valid_sets=[dtrain], \n",
    "                      valid_names=['train'], \n",
    "                      evals_result=evals_results, \n",
    "                      num_boost_round=OPT_ROUNDS,\n",
    "                      verbose_eval=50,\n",
    "                      feval=None)\n",
    "    \n",
    "del dtrain\n",
    "gc.collect()\n",
    "\n",
    "test_cols = ['ip','app','device','os', 'channel', 'click_time', 'click_id']\n",
    "test_df = pd.read_csv(path+\"test.csv\", dtype=dtypes, usecols=test_cols)\n",
    "test_df = prep_data(test_df)\n",
    "gc.collect()\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['click_id'] = test_df['click_id']\n",
    "sub['is_attributed'] = model.predict(test_df[predictors])\n",
    "sub.to_csv(output_filename, index=False, float_format='%.9f')\n",
    "\n",
    "print('=='*35)\n",
    "print('============================ Final Report ============================')\n",
    "print('=='*35)\n",
    "print(datetime.now(), '\\n')\n",
    "print('{:^17} : {:}'.format('train time', datetime.now()-start))\n",
    "print('{:^17} : {:}'.format('output file', output_filename))\n",
    "print('{:^17} : {:.5f}'.format('train auc', model.best_score['train']['auc']))\n",
    "if VALIDATE:\n",
    "    print('{:^17} : {:.5f}\\n'.format('valid auc', model.best_score['valid']['auc']))\n",
    "    print('{:^17} : {:}\\n{:^17} : {}\\n{:^17} : {}'.format('VALIDATE', VALIDATE, 'VALID_SIZE', VALID_SIZE, 'RANDOM_STATE', RANDOM_STATE))\n",
    "print('{:^17} : {:}\\n{:^17} : {}\\n{:^17} : {}\\n'.format('MAX_ROUNDS', MAX_ROUNDS, 'EARLY_STOP', EARLY_STOP, 'OPT_ROUNDS', model.best_iteration))\n",
    "print('{:^17} : {:}\\n{:^17} : {}\\n'.format('skiprows', skiprows, 'nrows', nrows))\n",
    "print('{:^17} : {:}\\n{:^17} : {}\\n'.format('variables', predictors, 'categorical', categorical))\n",
    "print('{:^17} : {:}\\n'.format('model params', params))\n",
    "print('=='*35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
