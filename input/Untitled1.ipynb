{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akhil Upadhyay\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train data...\n",
      "loading test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akhil Upadhyay\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6201: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting new features...\n",
      "grouping by ip-day-hour combination...\n",
      "group by ip-app combination...\n",
      "group by ip-app-os combination...\n",
      "merging...\n",
      "vars and data type: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18890469 entries, 0 to 18890468\n",
      "Data columns (total 15 columns):\n",
      "app                int64\n",
      "attributed_time    object\n",
      "channel            int64\n",
      "click_id           float64\n",
      "click_time         object\n",
      "device             int64\n",
      "ip                 int64\n",
      "is_attributed      float64\n",
      "os                 int64\n",
      "hour               uint8\n",
      "day                uint8\n",
      "wday               uint8\n",
      "qty                int64\n",
      "ip_app_count       int64\n",
      "ip_app_os_count    int64\n",
      "dtypes: float64(2), int64(8), object(2), uint8(3)\n",
      "memory usage: 1.9+ GB\n",
      "train size:  70000\n",
      "valid size:  30000\n",
      "test size :  18790469\n",
      "Training...\n",
      "preparing validation datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akhil Upadhyay\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttrain's auc: 0.870654\tvalid's auc: 0.79028\n",
      "[20]\ttrain's auc: 0.953936\tvalid's auc: 0.890791\n",
      "[30]\ttrain's auc: 0.976172\tvalid's auc: 0.909451\n",
      "[40]\ttrain's auc: 0.924218\tvalid's auc: 0.880225\n",
      "[50]\ttrain's auc: 0.857027\tvalid's auc: 0.780146\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttrain's auc: 0.954195\tvalid's auc: 0.97626\n",
      "\n",
      "Model Report\n",
      "n_estimators :  1\n",
      "auc: 0.9762601990166891\n",
      "[2.697518825531006]: model training time\n",
      "Predicting...\n",
      "writing...\n",
      "done...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The training data for this kernel was prepared by selecting only the records from 9th of November. \n",
    "\n",
    "The test dataset contains significant data for only the following hours of 10 Nov - 4,5,9,10,13,14. Check my EDA for the same - https://www.kaggle.com/gopisaran/indepth-eda-entire-talkingdata-dataset\n",
    "So I further filtered the Nov 9th data using those hours.\n",
    "\n",
    "This yields good AUC for the current 18% test set\n",
    "\n",
    "Used the following SQL to prepare training sample - fraud_sample.csv which consists of only 18 million records. \n",
    "\n",
    "select ip,app,device,os,channel,click_time,attributed_time,is_attributed from fraud where extract(day from click_time)=9 and \n",
    "extract(hour from click_time) in (4,5,9,10,13,14)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "def lgb_modelfit_nocv(params, dtrain, dvalid, predictors, target='target', objective='binary', metrics='auc',\n",
    "                 feval=None, early_stopping_rounds=20, num_boost_round=3000, verbose_eval=10, categorical_features=None):\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': objective,\n",
    "        'metric':metrics,\n",
    "        'learning_rate': 0.01,\n",
    "        #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "        'num_leaves': 31,  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': -1,  # -1 means no limit\n",
    "        'min_child_samples': 20,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 255,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.6,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 0,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.3,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 5,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 0,  # L1 regularization term on weights\n",
    "        'reg_lambda': 0,  # L2 regularization term on weights\n",
    "        'nthread': 4,\n",
    "        'verbose': 0,\n",
    "        'metric':metrics\n",
    "    }\n",
    "\n",
    "    lgb_params.update(params)\n",
    "\n",
    "    print(\"preparing validation datasets\")\n",
    "\n",
    "    xgtrain = lgb.Dataset(dtrain[predictors].values, label=dtrain[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "    xgvalid = lgb.Dataset(dvalid[predictors].values, label=dvalid[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    bst1 = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     valid_sets=[xgtrain, xgvalid], \n",
    "                     valid_names=['train','valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=num_boost_round,\n",
    "                     early_stopping_rounds=early_stopping_rounds,\n",
    "                     verbose_eval=10, \n",
    "                     feval=feval)\n",
    "\n",
    "    n_estimators = bst1.best_iteration\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"n_estimators : \", n_estimators)\n",
    "    print(metrics+\":\", evals_results['valid'][metrics][n_estimators-1])\n",
    "\n",
    "    return bst1\n",
    "\n",
    "path = '../input/'\n",
    "\n",
    "\n",
    "print('loading train data...')\n",
    "train_df = pd.read_csv(path+\"train.csv\")\n",
    "train_df.columns = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'attributed_time', 'is_attributed']\n",
    "\n",
    "print('loading test data...')\n",
    "train_df.head()\n",
    "test_df = pd.read_csv(path+\"test.csv\")\n",
    "test_df.head()\n",
    "len_train = len(train_df)\n",
    "train_df=train_df.append(test_df)\n",
    "\n",
    "del test_df\n",
    "gc.collect()\n",
    "\n",
    "print('Extracting new features...')\n",
    "train_df['hour'] = pd.to_datetime(train_df.click_time).dt.hour.astype('uint8')\n",
    "train_df['day'] = pd.to_datetime(train_df.click_time).dt.day.astype('uint8')\n",
    "train_df['wday']  = pd.to_datetime(train_df.click_time).dt.dayofweek.astype('uint8')\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print('grouping by ip-day-hour combination...')\n",
    "gp = train_df[['ip','day','hour','channel']].groupby(by=['ip','day','hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'qty'})\n",
    "train_df = train_df.merge(gp, on=['ip','day','hour'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "print('group by ip-app combination...')\n",
    "gp = train_df[['ip', 'app', 'channel']].groupby(by=['ip', 'app'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_app_count'})\n",
    "train_df = train_df.merge(gp, on=['ip','app'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "print('group by ip-app-os combination...')\n",
    "gp = train_df[['ip','app', 'os', 'channel']].groupby(by=['ip', 'app', 'os'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_app_os_count'})\n",
    "print(\"merging...\")\n",
    "train_df = train_df.merge(gp, on=['ip','app', 'os'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "print(\"vars and data type: \")\n",
    "train_df.info()\n",
    "train_df['qty'] = train_df['qty'].astype('uint16')\n",
    "train_df['ip_app_count'] = train_df['ip_app_count'].astype('uint16')\n",
    "train_df['ip_app_os_count'] = train_df['ip_app_os_count'].astype('uint16')\n",
    "\n",
    "test_df = train_df[len_train:]\n",
    "val_df = train_df[(len_train-30000):len_train]\n",
    "train_df = train_df[:(len_train-30000)]\n",
    "\n",
    "print(\"train size: \", len(train_df))\n",
    "print(\"valid size: \", len(val_df))\n",
    "print(\"test size : \", len(test_df))\n",
    "\n",
    "target = 'is_attributed'\n",
    "predictors = ['app','device','os', 'channel', 'hour', 'day', 'wday', 'qty', 'ip_app_count', 'ip_app_os_count']\n",
    "categorical = ['app', 'device', 'os', 'channel', 'hour', 'day', 'wday']\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['click_id'] = test_df['click_id'].astype('int')\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(\"Training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.15,\n",
    "    #'is_unbalance': 'true', # replaced with scale_pos_weight argument\n",
    "    'num_leaves': 15,  # 2^max_depth - 1\n",
    "    'max_depth': 4,  # -1 means no limit\n",
    "    'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "    'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "    'subsample': .7,  # Subsample ratio of the training instance.\n",
    "    'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "    'colsample_bytree': 0.7,  # Subsample ratio of columns when constructing each tree.\n",
    "    'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "    'scale_pos_weight':99 # because training data is extremely unbalanced \n",
    "}\n",
    "bst = lgb_modelfit_nocv(params, \n",
    "                        train_df, \n",
    "                        val_df, \n",
    "                        predictors, \n",
    "                        target, \n",
    "                        objective='binary', \n",
    "                        metrics='auc',\n",
    "                        early_stopping_rounds=50, \n",
    "                        verbose_eval=True, \n",
    "                        num_boost_round=350, \n",
    "                        categorical_features=categorical)\n",
    "\n",
    "print('[{}]: model training time'.format(time.time() - start_time))\n",
    "del train_df\n",
    "del val_df\n",
    "gc.collect()\n",
    "\n",
    "print(\"Predicting...\")\n",
    "sub['is_attributed'] = bst.predict(test_df[predictors])\n",
    "print(\"writing...\")\n",
    "sub.to_csv('sub_lgb_balanced99.csv',index=False)\n",
    "print(\"done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
